{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf085ed",
   "metadata": {},
   "source": [
    "# Predicting wild fires in California based on Environmental Conditions ðŸŒ²ðŸ”¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f667f10",
   "metadata": {},
   "source": [
    "Dataset used: https://www.openml.org/search?type=data&status=active&id=43606"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea054c7",
   "metadata": {},
   "source": [
    "### 1. Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5df12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import pi\n",
    "import re\n",
    "from io import StringIO\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30078203",
   "metadata": {},
   "source": [
    "### 2. Visualizing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2640cb5",
   "metadata": {},
   "source": [
    "Loading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ea1c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the ARFF file manually\n",
    "with open(\"./california-environmental-conditions-dataset.arff\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Find where @DATA starts\n",
    "data_start = 0\n",
    "for i, line in enumerate(lines):\n",
    "    if line.strip().upper() == '@DATA':\n",
    "        data_start = i + 1\n",
    "        break\n",
    "\n",
    "# Extract column names from @ATTRIBUTE lines\n",
    "columns = []\n",
    "for line in lines:\n",
    "    if line.strip().startswith('@ATTRIBUTE'):\n",
    "        # Extract attribute name (handle quoted names and names with parentheses)\n",
    "        match = re.search(r'@ATTRIBUTE\\s+(\".*?\"|\\S+)\\s+', line, re.IGNORECASE)\n",
    "        if match:\n",
    "            col_name = match.group(1).strip('\"')\n",
    "            columns.append(col_name)\n",
    "\n",
    "# Read data section into DataFrame\n",
    "data_lines = [line.strip() for line in lines[data_start:] if line.strip()]\n",
    "\n",
    "# Define data types for each column\n",
    "dtype_dict = {\n",
    "    'Stn_Id': 'int64',\n",
    "    'Stn_Name': 'str',\n",
    "    'CIMIS_Region': 'str',\n",
    "    'Date': 'str',\n",
    "    'ETo_(in)': 'float64',\n",
    "    'Precip_(in)': 'float64',\n",
    "    'Sol_Rad_(Ly/day)': 'float64',\n",
    "    'Avg_Vap_Pres_(mBars)': 'float64',\n",
    "    'Max_Air_Temp_(F)': 'float64',\n",
    "    'Min_Air_Temp_(F)': 'float64',\n",
    "    'Avg_Air_Temp_(F)': 'float64',\n",
    "    'Max_Rel_Hum_(%)': 'float64',\n",
    "    'Min_Rel_Hum_(%)': 'float64',\n",
    "    'Avg_Rel_Hum_(%)': 'float64',\n",
    "    'Dew_Point_(F)': 'float64',\n",
    "    'Avg_Wind_Speed_(mph)': 'float64',\n",
    "    'Wind_Run_(miles)': 'float64',\n",
    "    'Avg_Soil_Temp_(F)': 'float64',\n",
    "    'Target': 'int64'\n",
    "}\n",
    "\n",
    "# Parse CSV-like data (handling quotes and missing values marked as '?')\n",
    "data_str = '\\n'.join(data_lines)\n",
    "df = pd.read_csv(\n",
    "    StringIO(data_str),\n",
    "    names=columns,\n",
    "    quotechar=\"'\",\n",
    "    escapechar='\\\\',\n",
    "    dtype=dtype_dict,na_values='?',\n",
    "    low_memory=False\n",
    "    )\n",
    "\n",
    "print(f\"Data loaded successfully: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd7400",
   "metadata": {},
   "source": [
    "Data insights/visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human-readable descriptions based on dataset metadata\n",
    "description_map = {\n",
    "    'Target': 'Binary target indicating whether a fire occurred (0 = no, 1 = yes)',\n",
    "\n",
    "    'Stn_Id': 'CIMIS station ID (numeric identifier for weather station)',\n",
    "    'Stn_Name': 'Weather station name',\n",
    "    'CIMIS_Region': 'CIMIS region name',\n",
    "\n",
    "    'Date': 'Observation date (MM/DD/YYYY)',\n",
    "\n",
    "    'ETo_(in)': 'Reference evapotranspiration in inches',\n",
    "    'Precip_(in)': 'Precipitation in inches',\n",
    "\n",
    "    'Sol_Rad_(Ly/day)': 'Solar radiation (Ly/day)',\n",
    "\n",
    "    'Avg_Vap_Pres_(mBars)': 'Average vapor pressure in mBars',\n",
    "\n",
    "    'Max_Air_Temp_(F)': 'Maximum air temperature (Â°F)',\n",
    "    'Min_Air_Temp_(F)': 'Minimum air temperature (Â°F)',\n",
    "    'Avg_Air_Temp_(F)': 'Average air temperature (Â°F)',\n",
    "\n",
    "    'Max_Rel_Hum_(%)': 'Maximum relative humidity (%)',\n",
    "    'Min_Rel_Hum_(%)': 'Minimum relative humidity (%)',\n",
    "    'Avg_Rel_Hum_(%)': 'Average relative humidity (%)',\n",
    "\n",
    "    'Dew_Point_(F)': 'Dew point temperature (Â°F)',\n",
    "    'Avg_Wind_Speed_(mph)': 'Average wind speed (mph)',\n",
    "    'Wind_Run_(miles)': 'Wind run (miles) â€” cumulative distance traveled by wind',\n",
    "\n",
    "    'Avg_Soil_Temp_(F)': 'Average soil temperature (Â°F)'\n",
    "}\n",
    "\n",
    "summary = []\n",
    "for col in df.columns:\n",
    "\n",
    "    # Getting col\n",
    "    col_series = df[col]\n",
    "\n",
    "    # Getting col properties\n",
    "    dtype = str(col_series.dtype)\n",
    "    distinct = int(col_series.nunique(dropna=True))\n",
    "    missing = int(col_series.isnull().sum())\n",
    "\n",
    "    # Showing examples\n",
    "    examples = list(pd.Series(col_series.dropna().unique()).astype(str)[:3])\n",
    "    examples_str = \"<><>\".join(examples) if examples else \"\"\n",
    "    description = description_map.get(col, \"NO DESCRIPTION FOUND\")\n",
    "\n",
    "    # Building the list\n",
    "    summary.append({\n",
    "        'column': col,\n",
    "        'dtype': dtype,\n",
    "        'distinct_values': distinct,\n",
    "        'missing': missing,\n",
    "        'examples': examples_str,\n",
    "        'description': description\n",
    "    })\n",
    "\n",
    "# Converting list to DataFrame\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "# Numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['number'])\n",
    "numeric_cols_list = numeric_cols.columns.tolist()\n",
    "print(f\"Numeric columns ({len(numeric_cols_list)}): {numeric_cols_list}\")\n",
    "\n",
    "# Non-numeric columns  \n",
    "non_numeric_cols = df.select_dtypes(exclude=['number'])\n",
    "non_numeric_cols_list = non_numeric_cols.columns.tolist()\n",
    "print(f\"Non-numeric columns ({len(non_numeric_cols_list)}): {non_numeric_cols_list}\")\n",
    "\n",
    "# Displaying DataFrame\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a01690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1147f9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation\n",
    "corr = df.select_dtypes(include=[\"number\"]).corr()\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "sns.heatmap(corr, annot=True, cmap='viridis', fmt='.1f')\n",
    "plt.title('Correlation matrix (numeric features)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9310bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target variable\n",
    "plt.figure(figsize=(8, 5))\n",
    "target_counts = df[\"Target\"].value_counts()\n",
    "plt.bar(['No Fire', 'Fire'], target_counts.values, color=['green', 'red'], alpha=0.7)\n",
    "plt.title('Distribution of Target Variable (Fire Occurrence)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Fire Status')\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    plt.text(i, v + 1000, str(v), ha='center', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"No Fire (0): {target_counts[0]} ({target_counts[0]/len(df[\"Target\"])*100:.2f}%)\")\n",
    "print(f\"Fire (1): {target_counts[1]} ({target_counts[1]/len(df[\"Target\"])*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a1189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series: Convert date and analyze temporal patterns\n",
    "df['Date_parsed'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')\n",
    "df['Year'] = df['Date_parsed'].dt.year\n",
    "df['Month'] = df['Date_parsed'].dt.month\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Fire occurrences by year\n",
    "yearly_fires = df.groupby('Year')['Target'].agg(['sum', 'count'])\n",
    "yearly_fires['fire_rate'] = (yearly_fires['sum'] / yearly_fires['count'] * 100)\n",
    "\n",
    "axes[0].bar(yearly_fires.index, yearly_fires['sum'], color='darkred', alpha=0.7, label='Fire Count')\n",
    "ax2 = axes[0].twinx()\n",
    "ax2.plot(yearly_fires.index, yearly_fires['fire_rate'], color='orange', marker='o', linewidth=2, label='Fire Rate (%)')\n",
    "axes[0].set_xlabel('Year')\n",
    "axes[0].set_ylabel('Number of Fires', color='darkred')\n",
    "ax2.set_ylabel('Fire Rate (%)', color='orange')\n",
    "axes[0].set_title('Fire Occurrences by Year', fontsize=12, fontweight='bold')\n",
    "axes[0].tick_params(axis='y', labelcolor='darkred')\n",
    "ax2.tick_params(axis='y', labelcolor='orange')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Fire occurrences by month\n",
    "monthly_fires = df.groupby('Month')['Target'].agg(['sum', 'count'])\n",
    "monthly_fires['fire_rate'] = (monthly_fires['sum'] / monthly_fires['count'] * 100)\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "axes[1].bar(range(1, 13), monthly_fires['sum'], color='firebrick', alpha=0.7)\n",
    "axes[1].set_xticks(range(1, 13))\n",
    "axes[1].set_xticklabels(month_names)\n",
    "axes[1].set_xlabel('Month')\n",
    "axes[1].set_ylabel('Number of Fires')\n",
    "axes[1].set_title('Fire Occurrences by Month', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f594d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-numeric features: CIMIS Regions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Region distribution\n",
    "region_counts = df['CIMIS_Region'].value_counts()\n",
    "axes[0].bar(range(len(region_counts)), region_counts.values, color='mediumseagreen', alpha=0.7)\n",
    "axes[0].set_xticks(range(len(region_counts)))\n",
    "axes[0].set_xticklabels(region_counts.index, rotation=45, ha='right', fontsize=9)\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of CIMIS Regions', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Fire occurrence by region\n",
    "region_fire = df.groupby('CIMIS_Region')['Target'].agg(['sum', 'count'])\n",
    "region_fire['fire_rate'] = (region_fire['sum'] / region_fire['count'] * 100)\n",
    "region_fire = region_fire.sort_values('fire_rate', ascending=False)\n",
    "\n",
    "axes[1].bar(range(len(region_fire)), region_fire['fire_rate'].values, color='orangered', alpha=0.7)\n",
    "axes[1].set_xticks(range(len(region_fire)))\n",
    "axes[1].set_xticklabels(region_fire.index, rotation=45, ha='right', fontsize=9)\n",
    "axes[1].set_ylabel('Fire Occurrence Rate (%)')\n",
    "axes[1].set_title('Fire Occurrence Rate by CIMIS Region', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-numeric features: Station Names\n",
    "plt.figure(figsize=(12, 6))\n",
    "station_counts = df['Stn_Name'].value_counts().head(20)\n",
    "plt.barh(range(len(station_counts)), station_counts.values, color='coral', alpha=0.7)\n",
    "plt.yticks(range(len(station_counts)), station_counts.index, fontsize=9)\n",
    "plt.xlabel('Number of Observations')\n",
    "plt.title('Top 20 Weather Stations by Number of Observations', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a10762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for numeric features by target\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 18))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Exclude 'Target' from features to plot since we're grouping by Target\n",
    "numeric_features = [col for col in numeric_cols.columns.tolist() if col != 'Target']\n",
    "\n",
    "for idx, col in enumerate(numeric_features):\n",
    "    data_to_plot = [df[df['Target'] == 0][col].dropna(), df[df['Target'] == 1][col].dropna()]\n",
    "    axes[idx].boxplot(data_to_plot, tick_labels=['No Fire', 'Fire'])\n",
    "    axes[idx].set_title(col, fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Value')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95400cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numeric features\n",
    "fig, axes = plt.subplots(5, 3, figsize=(15, 18))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Exclude 'Target' from features to plot since we're grouping by Target\n",
    "numeric_features = [col for col in numeric_cols.columns.tolist() if col != 'Target']\n",
    "\n",
    "for idx, col in enumerate(numeric_features):\n",
    "    axes[idx].hist(df[col].dropna(), bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    axes[idx].set_title(col, fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Value')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aa6fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find features with high correlation (> 0.5 or < -0.5)\n",
    "# Get correlation matrix\n",
    "corr_matrix = numeric_cols.corr()\n",
    "\n",
    "# Find pairs with high correlation (excluding diagonal)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.5:\n",
    "            high_corr_pairs.append({\n",
    "                'feature1': corr_matrix.columns[i],\n",
    "                'feature2': corr_matrix.columns[j],\n",
    "                'correlation': corr_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "# Display high correlation pairs\n",
    "high_corr_df = pd.DataFrame(high_corr_pairs).sort_values('correlation', ascending=False, key=abs)\n",
    "print(f\"Found {len(high_corr_df)} feature pairs with |correlation| > 0.5:\\n\")\n",
    "high_corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d61718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed pairwise plots for top highly correlated pairs\n",
    "num_pairs = len(high_corr_pairs)\n",
    "if num_pairs > 0:\n",
    "    top_n = min(6, num_pairs)  # Show top 6 pairs\n",
    "    top_pairs = high_corr_df.head(top_n)\n",
    "    \n",
    "    fig, axes = plt.subplots(top_n, 3, figsize=(15, 4*top_n))\n",
    "    if top_n == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, (_, pair) in enumerate(top_pairs.iterrows()):\n",
    "        feat1 = pair['feature1']\n",
    "        feat2 = pair['feature2']\n",
    "        corr_val = pair['correlation']\n",
    "        \n",
    "        # Scatter plot with regression line\n",
    "        valid_data = df[[feat1, feat2]].dropna()\n",
    "        axes[idx, 0].scatter(valid_data[feat1], valid_data[feat2], alpha=0.3, s=10)\n",
    "        z = np.polyfit(valid_data[feat1], valid_data[feat2], 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(valid_data[feat1].min(), valid_data[feat1].max(), 100)\n",
    "        axes[idx, 0].plot(x_line, p(x_line), \"r--\", linewidth=2, label=f'y={z[0]:.2f}x+{z[1]:.2f}')\n",
    "        axes[idx, 0].set_xlabel(feat1, fontsize=9)\n",
    "        axes[idx, 0].set_ylabel(feat2, fontsize=9)\n",
    "        axes[idx, 0].set_title(f'Scatter: {feat1} vs {feat2}', fontsize=10, fontweight='bold')\n",
    "        axes[idx, 0].legend()\n",
    "        axes[idx, 0].grid(alpha=0.3)\n",
    "        \n",
    "        # Joint distribution by target\n",
    "        no_fire = df[df['Target'] == 0]\n",
    "        fire = df[df['Target'] == 1]\n",
    "        axes[idx, 1].scatter(no_fire[feat1], no_fire[feat2], alpha=0.3, s=10, label='No Fire', c='green')\n",
    "        axes[idx, 1].scatter(fire[feat1], fire[feat2], alpha=0.3, s=10, label='Fire', c='red')\n",
    "        axes[idx, 1].set_xlabel(feat1, fontsize=9)\n",
    "        axes[idx, 1].set_ylabel(feat2, fontsize=9)\n",
    "        axes[idx, 1].set_title(f'By Target (Corr: {corr_val:.3f})', fontsize=10, fontweight='bold')\n",
    "        axes[idx, 1].legend()\n",
    "        axes[idx, 1].grid(alpha=0.3)\n",
    "        \n",
    "        # Hexbin plot for density\n",
    "        hexbin = axes[idx, 2].hexbin(valid_data[feat1], valid_data[feat2], gridsize=30, cmap='YlOrRd', mincnt=1)\n",
    "        axes[idx, 2].set_xlabel(feat1, fontsize=9)\n",
    "        axes[idx, 2].set_ylabel(feat2, fontsize=9)\n",
    "        axes[idx, 2].set_title(f'Density Plot', fontsize=10, fontweight='bold')\n",
    "        plt.colorbar(hexbin, ax=axes[idx, 2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3402328a",
   "metadata": {},
   "source": [
    "### 3. Data cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f736032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25938d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unnecessary features\n",
    "df.drop(columns=\"Stn_Name\", inplace=True)\n",
    "\n",
    "# Parsing date to date object\n",
    "df['Date_parsed'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099953ed",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee79677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by region, station and date to ensure proper time series calculations\n",
    "df = df.sort_values(['CIMIS_Region', 'Stn_Id', 'Date_parsed']).reset_index(drop=True)\n",
    "\n",
    "print(\"Creating rolling average features based on Date...\")\n",
    "print(f\"Original dataframe shape: {df.shape}\")\n",
    "\n",
    "# Group by region and station for proper rolling calculations\n",
    "grouped = df.groupby(['CIMIS_Region', 'Stn_Id'])\n",
    "\n",
    "# ETo_(in) - previous day, 3 days, 5 days, 1 week, 2 weeks, 3 weeks\n",
    "for days in [1, 3, 5, 7, 14, 21]:\n",
    "    col_name = f'ETo_(in)_avg_{days}d'\n",
    "    df[col_name] = grouped['ETo_(in)'].transform(lambda x: x.shift(1).rolling(window=days).mean())\n",
    "    print(f\"Created: {col_name}\")\n",
    "\n",
    "# Precip_(in) - previous day, 3 days, 5 days, 1 week, 2 weeks, 3 weeks\n",
    "for days in [1, 3, 5, 7, 14, 21]:\n",
    "    col_name = f'Precip_(in)_avg_{days}d'\n",
    "    df[col_name] = grouped['Precip_(in)'].transform(lambda x: x.shift(1).rolling(window=days).mean())\n",
    "    print(f\"Created: {col_name}\")\n",
    "\n",
    "# Sol_Rad_(Ly/day) - previous day, 3 days, 5 days\n",
    "for days in [1, 3, 5]:\n",
    "    col_name = f'Sol_Rad_(Ly/day)_avg_{days}d'\n",
    "    df[col_name] = grouped['Sol_Rad_(Ly/day)'].transform(lambda x: x.shift(1).rolling(window=days).mean())\n",
    "    print(f\"Created: {col_name}\")\n",
    "\n",
    "# Avg_Vap_Pres_(mBars) - previous day, 3 days, 5 days\n",
    "for days in [1, 3, 5]:\n",
    "    col_name = f'Avg_Vap_Pres_(mBars)_avg_{days}d'\n",
    "    df[col_name] = grouped['Avg_Vap_Pres_(mBars)'].transform(lambda x: x.shift(1).rolling(window=days).mean())\n",
    "    print(f\"Created: {col_name}\")\n",
    "\n",
    "# Max_Air_Temp_(F) - 3 days, 5 days\n",
    "for days in [3, 5]:\n",
    "    col_name = f'Max_Air_Temp_(F)_avg_{days}d'\n",
    "    df[col_name] = grouped['Max_Air_Temp_(F)'].transform(lambda x: x.shift(1).rolling(window=days).mean())\n",
    "    print(f\"Created: {col_name}\")\n",
    "\n",
    "# Min_Air_Temp_(F) - 3 days, 5 days\n",
    "for days in [3, 5]:\n",
    "    col_name = f'Min_Air_Temp_(F)_avg_{days}d'\n",
    "    df[col_name] = grouped['Min_Air_Temp_(F)'].transform(lambda x: x.shift(1).rolling(window=days).mean())\n",
    "    print(f\"Created: {col_name}\")\n",
    "\n",
    "# Avg_Air_Temp_(F) - 3 days, 5 days\n",
    "for days in [3, 5]:\n",
    "    col_name = f'Avg_Air_Temp_(F)_avg_{days}d'\n",
    "    df[col_name] = grouped['Avg_Air_Temp_(F)'].transform(lambda x: x.shift(1).rolling(window=days).mean())\n",
    "    print(f\"Created: {col_name}\")\n",
    "\n",
    "# Max_Rel_Hum_(%) - 3 days, 5 days, 1 week, 2 weeks, 3 weeks\n",
    "for days in [3, 5, 7, 14, 21]:\n",
    "    col_name = f'Max_Rel_Hum_(%)_avg_{days}d'\n",
    "    df[col_name] = grouped['Max_Rel_Hum_(%)'].transform(lambda x: x.shift(1).rolling(window=days).mean())\n",
    "    print(f\"Created: {col_name}\")\n",
    "\n",
    "# Min_Rel_Hum_(%) - 3 days, 5 days, 1 week, 2 weeks, 3 weeks\n",
    "for days in [3, 5, 7, 14, 21]:\n",
    "    col_name = f'Min_Rel_Hum_(%)_avg_{days}d'\n",
    "    df[col_name] = grouped['Min_Rel_Hum_(%)'].transform(lambda x: x.shift(1).rolling(window=days).mean())\n",
    "    print(f\"Created: {col_name}\")\n",
    "\n",
    "# Avg_Rel_Hum_(%) - 3 days, 5 days, 1 week, 2 weeks, 3 weeks\n",
    "for days in [3, 5, 7, 14, 21]:\n",
    "    col_name = f'Avg_Rel_Hum_(%)_avg_{days}d'\n",
    "    df[col_name] = grouped['Avg_Rel_Hum_(%)'].transform(lambda x: x.shift(1).rolling(window=days).mean())\n",
    "    print(f\"Created: {col_name}\")\n",
    "\n",
    "# Avg_Soil_Temp_(F) - 3 days, 5 days\n",
    "for days in [3, 5]:\n",
    "    col_name = f'Avg_Soil_Temp_(F)_avg_{days}d'\n",
    "    df[col_name] = grouped['Avg_Soil_Temp_(F)'].transform(lambda x: x.shift(1).rolling(window=days).mean())\n",
    "    print(f\"Created: {col_name}\")\n",
    "\n",
    "print(f\"\\nFinal dataframe shape: {df.shape}\")\n",
    "print(f\"Total new features created: {df.shape[1] - 19}\")  # 19 was the original number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354eca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample data with some new features\n",
    "sample_cols = ['Date', 'CIMIS_Region', 'Stn_Id', 'ETo_(in)', 'ETo_(in)_avg_1d', 'ETo_(in)_avg_3d', 'ETo_(in)_avg_7d']\n",
    "print(f\"\\nSample data with new features:\")\n",
    "df[sample_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5591970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of new features\n",
    "new_feature_cols = [col for col in df.columns if '_avg_' in col]\n",
    "\n",
    "# Check for missing values in new features\n",
    "new_feature_missing = df[new_feature_cols].isnull().sum()\n",
    "print(\"Missing values in new rolling average features:\")\n",
    "print(new_feature_missing[new_feature_missing > 0])\n",
    "\n",
    "# Visualize one example: ETo rolling averages\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Plot for a single station\n",
    "station_sample = df[df['Stn_Id'] == df['Stn_Id'].iloc[0]].head(60)\n",
    "\n",
    "axes[0].plot(station_sample['Date_parsed'], station_sample['ETo_(in)'], label='Original', marker='o', markersize=3)\n",
    "axes[0].plot(station_sample['Date_parsed'], station_sample['ETo_(in)_avg_1d'], label='1-day avg', alpha=0.7)\n",
    "axes[0].plot(station_sample['Date_parsed'], station_sample['ETo_(in)_avg_3d'], label='3-day avg', alpha=0.7)\n",
    "axes[0].plot(station_sample['Date_parsed'], station_sample['ETo_(in)_avg_7d'], label='7-day avg', alpha=0.7)\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('ETo (in)')\n",
    "axes[0].set_title('Example: ETo Rolling Averages (First 60 days of first station)', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot for temperature averages\n",
    "axes[1].plot(station_sample['Date_parsed'], station_sample['Avg_Air_Temp_(F)'], label='Original', marker='o', markersize=3)\n",
    "axes[1].plot(station_sample['Date_parsed'], station_sample['Avg_Air_Temp_(F)_avg_3d'], label='3-day avg', alpha=0.7)\n",
    "axes[1].plot(station_sample['Date_parsed'], station_sample['Avg_Air_Temp_(F)_avg_5d'], label='5-day avg', alpha=0.7)\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Avg Air Temp (Â°F)')\n",
    "axes[1].set_title('Example: Temperature Rolling Averages', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411f676f",
   "metadata": {},
   "source": [
    "Checking if parameters were correcly added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e769a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is an error when creatin parameters FOR SURE\n",
    "\n",
    "df[[\"CIMIS_Region\", \"Stn_Id\", \"Date\", \"ETo_(in)_avg_1d\", \"ETo_(in)_avg_21d\"]].sort_values(by=\"ETo_(in)_avg_1d\", na_position='first')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6c2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"CIMIS_Region\", \"Stn_Id\", \"Date\", \"ETo_(in)_avg_1d\", \"ETo_(in)_avg_21d\"]].loc[df[\"Stn_Id\"] == 245][20:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting first and last date after removing missing values\n",
    "print(\"First date before removing missing values:\", df[\"Date_parsed\"].head(1).to_string(index=False))\n",
    "print(\"Last date before removing missing values:\", df[\"Date_parsed\"].tail(1).to_string(index=False))\n",
    "print(\"\\nMissing values total:\", df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac09487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNDERSTAND WHY I AM DROPPING TIME SERIES AND THE DATE IS NOT CHANING, I AM DOING SOMETHING WRONG\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting first and last date after removing missing values\n",
    "print(\"First date after removing missing values:\", df[\"Date_parsed\"].head(1).to_string(index=False))\n",
    "print(\"Last date after removing missing values:\", df[\"Date_parsed\"].tail(1).to_string(index=False))\n",
    "print(\"\\nMissing values total:\", df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a0966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting \"CIMIS_Region\" to numbers using ordinal encoding\n",
    "encoder = OrdinalEncoder()\n",
    "encoded = encoder.fit_transform(df[[\"CIMIS_Region\"]])\n",
    "df[\"CIMIS_Region_encoded\"] = encoded  \n",
    "\n",
    "print(\"Ordinal encoding applied to CIMIS_Region:\")\n",
    "for i, region in enumerate(encoder.categories_[0]):\n",
    "    print(f\"  {region}: {i}\")\n",
    "print(f\"\\nNew column 'CIMIS_Region_encoded' created with shape: {df['CIMIS_Region_encoded'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df144c52",
   "metadata": {},
   "source": [
    "### 5. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating data and dropping not useful columns\n",
    "# Remove Target, date-related columns, and original CIMIS_Region (keep encoded version)\n",
    "X = df.drop(columns=[\"Target\", \"Date\", \"Date_parsed\", \"Year\", \"Month\", \"CIMIS_Region\"])\n",
    "y = df[\"Target\"]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns (first 10): {list(X.columns)[:10]}\")\n",
    "print(f\"\\nTarget distribution:\\n{y.value_counts()}\")\n",
    "\n",
    "# Verify all columns are numeric\n",
    "print(f\"\\nData types in X:\")\n",
    "print(X.dtypes.value_counts())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8519b126",
   "metadata": {},
   "source": [
    "### Neural Network Training Note\n",
    "\n",
    "The Neural Network with 50 fully connected layers (100 neurons each) is now integrated with the other models below. It will be trained together with Random Forest, AdaBoost, and Gradient Boosting for comprehensive comparison.\n",
    "\n",
    "**Neural Network Architecture:**\n",
    "- 50 hidden layers\n",
    "- 100 neurons per layer\n",
    "- ReLU activation\n",
    "- Adam optimizer\n",
    "- Early stopping enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea83d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results dictionary and scoring metrics\n",
    "results = {}\n",
    "\n",
    "# Define scoring metrics for cross-validation\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1'\n",
    "}\n",
    "\n",
    "print(\"Ready to train models with 3-fold cross-validation\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0a5a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Random Forest\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize model with hyperparameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,      # Number of trees\n",
    "    random_state=42,\n",
    "    n_jobs=-1,             # Use all CPU cores\n",
    "    max_depth=None,        # Unlimited depth\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1\n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "print(\"\\nPerforming 3-fold cross-validation...\")\n",
    "cv_results = cross_validate(rf_model, X_train, y_train, cv=3, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "# Store CV results\n",
    "results['Random Forest'] = {\n",
    "    'cv_accuracy': cv_results['test_accuracy'],\n",
    "    'cv_precision': cv_results['test_precision'],\n",
    "    'cv_recall': cv_results['test_recall'],\n",
    "    'cv_f1': cv_results['test_f1']\n",
    "}\n",
    "\n",
    "# Print CV results\n",
    "print(f\"\\nCross-Validation Results:\")\n",
    "print(f\"  Accuracy:  {cv_results['test_accuracy'].mean():.4f} (+/- {cv_results['test_accuracy'].std():.4f})\")\n",
    "print(f\"  Precision: {cv_results['test_precision'].mean():.4f} (+/- {cv_results['test_precision'].std():.4f})\")\n",
    "print(f\"  Recall:    {cv_results['test_recall'].mean():.4f} (+/- {cv_results['test_recall'].std():.4f})\")\n",
    "print(f\"  F1-Score:  {cv_results['test_f1'].mean():.4f} (+/- {cv_results['test_f1'].std():.4f})\")\n",
    "\n",
    "# Train on full training set\n",
    "print(f\"\\nTraining on full training set...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_precision = precision_score(y_test, y_pred)\n",
    "test_recall = recall_score(y_test, y_pred)\n",
    "test_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Store test results\n",
    "results['Random Forest']['test_accuracy'] = test_accuracy\n",
    "results['Random Forest']['test_precision'] = test_precision\n",
    "results['Random Forest']['test_recall'] = test_recall\n",
    "results['Random Forest']['test_f1'] = test_f1\n",
    "results['Random Forest']['model'] = rf_model\n",
    "\n",
    "# Print test results\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(f\"  Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall:    {test_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0fbee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: AdaBoost\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADABOOST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize model with hyperparameters\n",
    "ada_model = AdaBoostClassifier(\n",
    "    n_estimators=100,      # Number of boosting stages\n",
    "    random_state=42,\n",
    "    learning_rate=1.0,     # Weight applied to each classifier\n",
    "    algorithm='SAMME'      # Algorithm type\n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "print(\"\\nPerforming 3-fold cross-validation...\")\n",
    "cv_results = cross_validate(ada_model, X_train, y_train, cv=3, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "# Store CV results\n",
    "results['AdaBoost'] = {\n",
    "    'cv_accuracy': cv_results['test_accuracy'],\n",
    "    'cv_precision': cv_results['test_precision'],\n",
    "    'cv_recall': cv_results['test_recall'],\n",
    "    'cv_f1': cv_results['test_f1']\n",
    "}\n",
    "\n",
    "# Print CV results\n",
    "print(f\"\\nCross-Validation Results:\")\n",
    "print(f\"  Accuracy:  {cv_results['test_accuracy'].mean():.4f} (+/- {cv_results['test_accuracy'].std():.4f})\")\n",
    "print(f\"  Precision: {cv_results['test_precision'].mean():.4f} (+/- {cv_results['test_precision'].std():.4f})\")\n",
    "print(f\"  Recall:    {cv_results['test_recall'].mean():.4f} (+/- {cv_results['test_recall'].std():.4f})\")\n",
    "print(f\"  F1-Score:  {cv_results['test_f1'].mean():.4f} (+/- {cv_results['test_f1'].std():.4f})\")\n",
    "\n",
    "# Train on full training set\n",
    "print(f\"\\nTraining on full training set...\")\n",
    "ada_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on test set\n",
    "y_pred = ada_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_precision = precision_score(y_test, y_pred)\n",
    "test_recall = recall_score(y_test, y_pred)\n",
    "test_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Store test results\n",
    "results['AdaBoost']['test_accuracy'] = test_accuracy\n",
    "results['AdaBoost']['test_precision'] = test_precision\n",
    "results['AdaBoost']['test_recall'] = test_recall\n",
    "results['AdaBoost']['test_f1'] = test_f1\n",
    "results['AdaBoost']['model'] = ada_model\n",
    "\n",
    "# Print test results\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(f\"  Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall:    {test_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b445c709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Gradient Boosting\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GRADIENT BOOSTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize model with hyperparameters\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,      # Number of boosting stages\n",
    "    random_state=42,\n",
    "    learning_rate=0.1,     # Shrinks contribution of each tree\n",
    "    max_depth=3,           # Maximum depth of individual trees\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    subsample=1.0          # Fraction of samples for fitting trees\n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "print(\"\\nPerforming 3-fold cross-validation...\")\n",
    "cv_results = cross_validate(gb_model, X_train, y_train, cv=3, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "# Store CV results\n",
    "results['Gradient Boosting'] = {\n",
    "    'cv_accuracy': cv_results['test_accuracy'],\n",
    "    'cv_precision': cv_results['test_precision'],\n",
    "    'cv_recall': cv_results['test_recall'],\n",
    "    'cv_f1': cv_results['test_f1']\n",
    "}\n",
    "\n",
    "# Print CV results\n",
    "print(f\"\\nCross-Validation Results:\")\n",
    "print(f\"  Accuracy:  {cv_results['test_accuracy'].mean():.4f} (+/- {cv_results['test_accuracy'].std():.4f})\")\n",
    "print(f\"  Precision: {cv_results['test_precision'].mean():.4f} (+/- {cv_results['test_precision'].std():.4f})\")\n",
    "print(f\"  Recall:    {cv_results['test_recall'].mean():.4f} (+/- {cv_results['test_recall'].std():.4f})\")\n",
    "print(f\"  F1-Score:  {cv_results['test_f1'].mean():.4f} (+/- {cv_results['test_f1'].std():.4f})\")\n",
    "\n",
    "# Train on full training set\n",
    "print(f\"\\nTraining on full training set...\")\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on test set\n",
    "y_pred = gb_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_precision = precision_score(y_test, y_pred)\n",
    "test_recall = recall_score(y_test, y_pred)\n",
    "test_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Store test results\n",
    "results['Gradient Boosting']['test_accuracy'] = test_accuracy\n",
    "results['Gradient Boosting']['test_precision'] = test_precision\n",
    "results['Gradient Boosting']['test_recall'] = test_recall\n",
    "results['Gradient Boosting']['test_f1'] = test_f1\n",
    "results['Gradient Boosting']['model'] = gb_model\n",
    "\n",
    "# Print test results\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(f\"  Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall:    {test_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6d1618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Neural Network (50 layers)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEURAL NETWORK (50 LAYERS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize model with hyperparameters\n",
    "nn_model = MLPClassifier(\n",
    "    hidden_layer_sizes=tuple([100] * 90),  # 90 hidden layers with 100 neurons each\n",
    "    activation='relu',                      # Activation function\n",
    "    solver='adam',                          # Optimizer\n",
    "    max_iter=200,                           # Maximum iterations\n",
    "    random_state=42,\n",
    "    verbose=True,                           # Show training progress\n",
    "    early_stopping=True,                    # Stop when validation score stops improving\n",
    "    validation_fraction=0.1,                # Fraction of training data for validation\n",
    "    learning_rate_init=0.001,               # Initial learning rate\n",
    "    batch_size='auto',                      # Batch size for training\n",
    "    alpha=0.0001                            # L2 regularization parameter\n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "print(\"\\nPerforming 3-fold cross-validation...\")\n",
    "cv_results = cross_validate(nn_model, X_train, y_train, cv=3, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "# Store CV results\n",
    "results['Neural Network (50 layers)'] = {\n",
    "    'cv_accuracy': cv_results['test_accuracy'],\n",
    "    'cv_precision': cv_results['test_precision'],\n",
    "    'cv_recall': cv_results['test_recall'],\n",
    "    'cv_f1': cv_results['test_f1']\n",
    "}\n",
    "\n",
    "# Print CV results\n",
    "print(f\"\\nCross-Validation Results:\")\n",
    "print(f\"  Accuracy:  {cv_results['test_accuracy'].mean():.4f} (+/- {cv_results['test_accuracy'].std():.4f})\")\n",
    "print(f\"  Precision: {cv_results['test_precision'].mean():.4f} (+/- {cv_results['test_precision'].std():.4f})\")\n",
    "print(f\"  Recall:    {cv_results['test_recall'].mean():.4f} (+/- {cv_results['test_recall'].std():.4f})\")\n",
    "print(f\"  F1-Score:  {cv_results['test_f1'].mean():.4f} (+/- {cv_results['test_f1'].std():.4f})\")\n",
    "\n",
    "# Train on full training set\n",
    "print(f\"\\nTraining on full training set...\")\n",
    "nn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on test set\n",
    "y_pred = nn_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "test_precision = precision_score(y_test, y_pred)\n",
    "test_recall = recall_score(y_test, y_pred)\n",
    "test_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Store test results\n",
    "results['Neural Network (50 layers)']['test_accuracy'] = test_accuracy\n",
    "results['Neural Network (50 layers)']['test_precision'] = test_precision\n",
    "results['Neural Network (50 layers)']['test_recall'] = test_recall\n",
    "results['Neural Network (50 layers)']['test_f1'] = test_f1\n",
    "results['Neural Network (50 layers)']['model'] = nn_model\n",
    "\n",
    "# Print test results\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(f\"  Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall:    {test_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ff78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: All models trained\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETED FOR ALL 4 MODELS!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModels trained: {list(results.keys())}\")\n",
    "print(\"\\nYou can now run the comparison and visualization cells below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd544010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison DataFrame for all 4 models\n",
    "comparison_data = []\n",
    "model_names = list(results.keys())\n",
    "\n",
    "for model_name in model_names:\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'CV Accuracy (mean)': results[model_name]['cv_accuracy'].mean(),\n",
    "        'CV Precision (mean)': results[model_name]['cv_precision'].mean(),\n",
    "        'CV Recall (mean)': results[model_name]['cv_recall'].mean(),\n",
    "        'CV F1-Score (mean)': results[model_name]['cv_f1'].mean(),\n",
    "        'Test Accuracy': results[model_name]['test_accuracy'],\n",
    "        'Test Precision': results[model_name]['test_precision'],\n",
    "        'Test Recall': results[model_name]['test_recall'],\n",
    "        'Test F1-Score': results[model_name]['test_f1']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nModel Comparison Summary (All 4 Models):\")\n",
    "print(\"=\"*80)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c295e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance comparison (All 4 Models)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "metric_keys = ['accuracy', 'precision', 'recall', 'f1']  # Correct metric keys for results dict\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Added 4th color for neural network\n",
    "\n",
    "model_names = list(results.keys())\n",
    "\n",
    "for idx, (metric, metric_key) in enumerate(zip(metrics, metric_keys)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Get CV and Test scores using correct metric keys\n",
    "    cv_scores = [results[model][f'cv_{metric_key}'].mean() for model in model_names]\n",
    "    test_scores = [results[model][f'test_{metric_key}'] for model in model_names]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, cv_scores, width, label='CV Mean', alpha=0.8, color=colors)\n",
    "    bars2 = ax.bar(x + width/2, test_scores, width, label='Test', alpha=0.8, color=[c for c in colors])\n",
    "    \n",
    "    # Add error bars for CV scores\n",
    "    cv_stds = [results[model][f'cv_{metric_key}'].std() for model in model_names]\n",
    "    ax.errorbar(x - width/2, cv_scores, yerr=cv_stds, fmt='none', ecolor='black', capsize=5, alpha=0.6)\n",
    "    \n",
    "    ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names, rotation=15, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8046a6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrices for all 4 models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "for idx, (model_name, model_data) in enumerate(results.items()):\n",
    "    model = model_data['model']\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix in 2x2 grid\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, \n",
    "                cbar_kws={'label': 'Count'})\n",
    "    ax.set_title(f'{model_name}\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_xticklabels(['No Fire', 'Fire'])\n",
    "    ax.set_yticklabels(['No Fire', 'Fire'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8169bc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification reports\n",
    "print(\"Detailed Classification Reports\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, model_data in results.items():\n",
    "    model = model_data['model']\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"\\n{model_name}\")\n",
    "    print(\"-\"*80)\n",
    "    print(classification_report(y_test, y_pred, target_names=['No Fire', 'Fire'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04691db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Visualization 1: Side-by-side metric comparison\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "metric_keys = ['accuracy', 'precision', 'recall', 'f1']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "model_names = list(results.keys())\n",
    "\n",
    "for idx, (metric, metric_key) in enumerate(zip(metrics, metric_keys)):\n",
    "    test_scores = [results[model][f'test_{metric_key}'] for model in model_names]\n",
    "    \n",
    "    bars = axes[idx].bar(range(len(model_names)), test_scores, color=colors, alpha=0.8)\n",
    "    axes[idx].set_title(f'Test {metric}', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Score', fontsize=12)\n",
    "    axes[idx].set_xticks(range(len(model_names)))\n",
    "    axes[idx].set_xticklabels([name.replace(' (50 layers)', '\\n(50 layers)') for name in model_names], \n",
    "                                rotation=0, ha='center', fontsize=9)\n",
    "    axes[idx].set_ylim([0, 1.05])\n",
    "    axes[idx].grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[idx].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726d7ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Visualization 2: Radar chart comparing all models\n",
    "fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "num_vars = len(categories)\n",
    "\n",
    "# Compute angle for each axis\n",
    "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Plot data for each model\n",
    "for idx, (model_name, model_data) in enumerate(results.items()):\n",
    "    values = [\n",
    "        model_data['test_accuracy'],\n",
    "        model_data['test_precision'],\n",
    "        model_data['test_recall'],\n",
    "        model_data['test_f1']\n",
    "    ]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[idx])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors[idx])\n",
    "\n",
    "# Set category labels\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, size=12, fontweight='bold')\n",
    "\n",
    "# Set y-axis limits and labels\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], size=10)\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "plt.title('Model Performance Comparison - Radar Chart', size=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a7688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Visualization 3: Heatmap of all metrics\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Prepare data for heatmap\n",
    "heatmap_data = []\n",
    "model_names = []\n",
    "for model_name, model_data in results.items():\n",
    "    model_names.append(model_name)\n",
    "    heatmap_data.append([\n",
    "        model_data['test_accuracy'],\n",
    "        model_data['test_precision'],\n",
    "        model_data['test_recall'],\n",
    "        model_data['test_f1']\n",
    "    ])\n",
    "\n",
    "heatmap_df = pd.DataFrame(heatmap_data, \n",
    "                            columns=['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "                            index=model_names)\n",
    "\n",
    "sns.heatmap(heatmap_df, annot=True, fmt='.4f', cmap='RdYlGn', \n",
    "            vmin=0.5, vmax=1.0, center=0.75,\n",
    "            linewidths=2, linecolor='white',\n",
    "            cbar_kws={'label': 'Score'},\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_title('Model Performance Heatmap (Test Set)', fontsize=16, fontweight='bold', pad=15)\n",
    "ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Models', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Visualization 4: Model ranking\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Calculate average performance across all metrics\n",
    "model_avg_scores = {}\n",
    "for model_name, model_data in results.items():\n",
    "    avg_score = (model_data['test_accuracy'] + \n",
    "                model_data['test_precision'] + \n",
    "                model_data['test_recall'] + \n",
    "                model_data['test_f1']) / 4\n",
    "    model_avg_scores[model_name] = avg_score\n",
    "\n",
    "# Sort models by average score\n",
    "sorted_models = sorted(model_avg_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "model_names_sorted = [item[0] for item in sorted_models]\n",
    "avg_scores = [item[1] for item in sorted_models]\n",
    "\n",
    "# Create bar chart\n",
    "bars = ax.barh(range(len(model_names_sorted)), avg_scores, \n",
    "                color=['gold', 'silver', '#CD7F32', 'lightcoral'][:len(model_names_sorted)],\n",
    "                alpha=0.8, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax.set_yticks(range(len(model_names_sorted)))\n",
    "ax.set_yticklabels(model_names_sorted, fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Average Score (across all metrics)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Model Ranking by Average Performance', fontsize=16, fontweight='bold', pad=15)\n",
    "ax.set_xlim([0, 1.05])\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels and ranking\n",
    "for idx, (bar, score) in enumerate(zip(bars, avg_scores)):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width + 0.01, bar.get_y() + bar.get_height()/2.,\n",
    "            f'{score:.4f}', ha='left', va='center', fontsize=11, fontweight='bold')\n",
    "    ax.text(0.01, bar.get_y() + bar.get_height()/2.,\n",
    "            f'#{idx+1}', ha='left', va='center', fontsize=12, fontweight='bold', color='white',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='black', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL RANKING\")\n",
    "print(\"=\"*80)\n",
    "for idx, (model_name, score) in enumerate(sorted_models, 1):\n",
    "    print(f\"#{idx}: {model_name:<30} - Average Score: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
